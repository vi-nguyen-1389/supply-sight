{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03523dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5ac83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe7bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792dcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b326ee7e",
   "metadata": {},
   "source": [
    "## B. Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bb33e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# import functions from the function module\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFunction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mProject_VNg63984_Function\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_manual, select_variance, select_best, make_poly, \\\n\u001b[1;32m     31\u001b[0m perform_linear_regression, perform_ridge_regression, perform_lasso_regression, calculate_sil, perform_regression\n\u001b[1;32m     33\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Function'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression,SelectFromModel, RFE\n",
    "from sklearn.metrics import mean_squared_error, silhouette_score, calinski_harabasz_score, davies_bouldin_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch, SpectralClustering\n",
    "from kneed import KneeLocator\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# import functions from the function module\n",
    "from Function.Project_VNg63984_Function import select_manual, select_variance, select_best, make_poly, \\\n",
    "perform_linear_regression, perform_ridge_regression, perform_lasso_regression, calculate_sil, perform_regression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b447",
   "metadata": {},
   "source": [
    "## C. Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ff12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "df = pd.read_csv(\"Dataset/DataCoSupplyChainDataset.csv\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3099706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetune the column name\n",
    "df.columns = df.columns.str.lower().str.replace(\" \", \"_\").str.replace(r\"\\(|\\)\", \"\").str.strip()\n",
    "\n",
    "#show the df again\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aeadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the dataset's info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing null value\n",
    "df.dropna(axis = 1, how = 'all', inplace = True )\n",
    "df.dropna(axis = 0, how = 'all', inplace = True)\n",
    "\n",
    "#drop zipcode and name columns \n",
    "df.drop(['order_zipcode','customer_zipcode','customer_fname','customer_lname'],axis = 1, inplace = True)\n",
    "\n",
    "#drop duplicates\n",
    "df.drop_duplicates(keep ='last', inplace = True)\n",
    "\n",
    "#check for null value\n",
    "print(f\"The number of null records is: {df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop uneccessary columns \n",
    "columns_to_drop = [\n",
    "    \"category_id\", \"customer_id\", \"customer_email\", \"customer_password\", \"department_id\", \"order_customer_id\", \n",
    "    \"order_id\",\"order_item_cardprod_id\", \"order_item_id\", \"product_card_id\", \"product_category_id\",\n",
    "    \"product_image\",\"product_status\",\"product_name\",\"customer_street\",]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the df's info again\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cacbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the data's first 5 rows again\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10)) \n",
    "\n",
    "sns.heatmap(df.corr(),\n",
    "           cbar=True,\n",
    "           annot=True,\n",
    "           square=True,   \n",
    "           fmt='.1g',\n",
    "           linewidths=0.5,\n",
    "           annot_kws={'size': 8},\n",
    "           cmap = \"Blues\"\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88962d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_sales = df.groupby(\"market\")[\"sales\"].sum().reset_index().sort_values(by=\"sales\", ascending=False)\n",
    "\n",
    "# create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(market_sales.market, market_sales.sales,color=['steelblue','lightblue'])\n",
    "plt.xlabel(\"Market\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales per Market\")\n",
    "plt.xticks(rotation=45, ha=\"right\")  \n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc38e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_sales = df.groupby(\"order_region\")[\"sales\"].sum().reset_index().sort_values(by=\"sales\", ascending=False)\n",
    "\n",
    "# create the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(region_sales.order_region, region_sales.sales,color=['green','darkgreen'])\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales per Region\")\n",
    "plt.xticks(rotation=45, ha=\"right\")  \n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af236a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(data_frame = df,\n",
    "                       lat = df.latitude,\n",
    "                       lon = df.longitude,\n",
    "                       color = df.sales,\n",
    "                       size = df.sales,\n",
    "                       hover_name = 'sales',\n",
    "                       hover_data = ['latitude','longitude'],\n",
    "                       title = 'Sales Distribution',\n",
    "                       mapbox_style = 'carto-positron')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='delivery_status')\n",
    "plt.title(\"Delivery Status Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b96133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461729b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1948ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee7a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65c90818",
   "metadata": {},
   "source": [
    "## D. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839de068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataset to a new one to start transforming\n",
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing pairwise correlation of columns\n",
    "corr = df_cleaned.corr()\n",
    "\n",
    "# create a list for columns to be dropped due to high correlation\n",
    "drop_columns = []\n",
    "\n",
    "# identifying highly correlated columns\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i, j] >= 0.9:\n",
    "            colname = corr.columns[j]\n",
    "            if colname not in drop_columns and colname != 'sales':\n",
    "                drop_columns.append(colname)\n",
    "\n",
    "# dropping highly correlated columns\n",
    "df_cleaned.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55345906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show boxplot to look for outliers\n",
    "sns.boxplot(data = df_cleaned, y = 'sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate outlier from the target\n",
    "Q1 = df_cleaned.sales.quantile(0.25)\n",
    "Q3 = df_cleaned.sales.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# remove outliers\n",
    "df_cleaned = df_cleaned[(df_cleaned.sales >= lower_bound) & (df_cleaned.sales <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb44a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show boxplot again\n",
    "sns.boxplot(data = df_cleaned, y = 'sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.corr().sales.abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of categorical columns\n",
    "cat_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
    "\n",
    "# show the list\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3763c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to store the unique values\n",
    "unique_values_df = pd.DataFrame(columns=['Column', 'Unique values','Unique value counts'])\n",
    "\n",
    "# iterate over each categorical column and retrieve unique values\n",
    "for column in cat_columns:\n",
    "    unique_values = df_cleaned[column].unique()\n",
    "    unique_values_count = len(df_cleaned[column].unique())\n",
    "    unique_values_df = unique_values_df.append({'Column': column, 'Unique values': unique_values, 'Unique value counts': unique_values_count}, ignore_index=True)\n",
    "\n",
    "# Print the dataframe\n",
    "unique_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all the values in categorical columns to replace hypen ‘-‘ into underscore and space to undersocre\n",
    "df_cleaned[cat_columns] = df_cleaned[cat_columns]\\\n",
    ".apply(lambda x: x.str.strip().str.replace(\"-\",\"_\").str.replace(\" \",\"_\").str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c499a",
   "metadata": {},
   "source": [
    "---\n",
    "**Rationale:**\n",
    "- **Granularity vs. Utility**: `customer_city` and `order_city` have high cardinality which might lead to overfitting and longer training times.\n",
    "- **Redundancy**: `market` and `order_region` likely capture essential geographical trends, making city and state columns potentially redundant.\n",
    "- **Model Complexity**: High cardinality columns can lead to increased feature space post-encoding, making models more complex and prone to overfitting.\n",
    "\n",
    "Considering these factors, we're dropping `customer_city`, `order_city`, and `order_state`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9267e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping 'customer_city', 'order_city', and 'order_state' \n",
    "df_cleaned = df_cleaned.drop(['customer_city', 'order_city', 'order_state','order_country'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34975a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check country column for unique value\n",
    "print(\"Unique value counts:\\n\")\n",
    "print(df_cleaned.customer_state.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88404fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the customer_state into customer_region\n",
    "northeast = ['me', 'nh', 'vt', 'ma', 'ct', 'ri', 'ny', 'nj', 'pa']\n",
    "midwest = ['oh', 'mi', 'in', 'wi', 'il', 'mn', 'ia', 'mo', 'nd', 'sd', 'ne', 'ks']\n",
    "south = ['de', 'md', 'dc', 'va', 'wv', 'nc', 'sc', 'ga', 'fl', 'ky', 'tn', 'al', 'ms', 'ar', 'la', 'tx', 'ok']\n",
    "west = ['mt', 'id', 'wy', 'co', 'nm', 'az', 'ut', 'nv', 'ca', 'or', 'wa', 'ak', 'hi']\n",
    "\n",
    "df_cleaned['customer_region'] = ['northeast' if state in northeast else 'midwest' if state in midwest \\\n",
    "                                 else 'south' if state in south else 'west' \\\n",
    "                                 if state in west else 'territories' if state == 'pr' \\\n",
    "                                 else 'other' for state in df_cleaned.customer_state]\n",
    "\n",
    "df_cleaned.drop('customer_state', axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check category_name column for unique value\n",
    "print(\"Unique value counts:\\n\")\n",
    "print(df_cleaned.category_name.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2d78b",
   "metadata": {},
   "source": [
    "The `department_name` column is found to be similar to the `category_name` column, adding potential redundancy to our dataset. To maintain simplicity and avoid multicollinearity, we've decided to drop the `category_name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15383616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.drop('category_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc89c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check department_name column for unique value\n",
    "print(\"Unique value counts:\\n\")\n",
    "print(df_cleaned.department_name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce complexity of department_name column:\n",
    "\n",
    "department_mapping = {\n",
    "    'fan_shop': 'sports_&_outdoor',\n",
    "    'golf': 'sports_&_outdoor',\n",
    "    'outdoors': 'sports_&_outdoor',\n",
    "    'fitness': 'sports_&_outdoor',\n",
    "    'apparel': 'apparel',\n",
    "    'footwear': 'footwear',\n",
    "    'discs_shop': 'entertainment',\n",
    "    'book_shop': 'entertainment',\n",
    "    'technology': 'technology',\n",
    "    'pet_shop': 'personal_care_&_lifestyle',\n",
    "    'health_and_beauty': 'personal_care_&_lifestyle'\n",
    "}\n",
    "\n",
    "df_cleaned['department_name'] = df_cleaned['department_name'].map(department_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf99c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check departmant_name column again\n",
    "print(df_cleaned.department_name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check order_region column for unique value\n",
    "print(\"Unique value counts:\\n\")\n",
    "print(df_cleaned.order_region.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce complexity of order_region\n",
    "order_region_mapping = {\n",
    "    'central_america': 'latin_america',\n",
    "    'south_america': 'latin_america',\n",
    "    'caribbean': 'latin_america',\n",
    "    \n",
    "    'west_of_usa': 'north_america',\n",
    "    'east_of_usa': 'north_america',\n",
    "    'us_center': 'north_america',\n",
    "    'south_of_usa': 'north_america',\n",
    "    'canada': 'north_america',\n",
    "    \n",
    "    'western_europe': 'europe',\n",
    "    'northern_europe': 'europe',\n",
    "    'southern_europe': 'europe',\n",
    "    'eastern_europe': 'europe',\n",
    "    \n",
    "    'southeast_asia': 'east_asia',\n",
    "    'eastern_asia': 'east_asia',\n",
    "    \n",
    "    'south_asia': 'south_asia',\n",
    "    \n",
    "    'west_asia': 'west_asia',\n",
    "    'central_asia': 'west_asia',\n",
    "    \n",
    "    'west_africa': 'africa',\n",
    "    'north_africa': 'africa',\n",
    "    'east_africa': 'africa',\n",
    "    'central_africa': 'africa',\n",
    "    'southern_africa': 'africa',\n",
    "    \n",
    "    'oceania': 'oceania'\n",
    "}\n",
    "\n",
    "df_cleaned.order_region = df_cleaned['order_region'].map(order_region_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261df8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check order_region column again\n",
    "print(df_cleaned.order_region.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47554105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check customer_country column for unique value\n",
    "print(\"Unique value counts:\\n\")\n",
    "print(df_cleaned.customer_country.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c316e28",
   "metadata": {},
   "source": [
    "The \"EE. UU.\" is the Spanish abbreviation for \"Estados Unidos,\" which translates to \"United States\" in English. To make it more intuitive for an English-speaking audience, we will change \"EE. UU.\" to \"united_states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a837d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing 'EE. UU.' with 'united_states'\n",
    "df_cleaned.customer_country = df_cleaned.customer_country.replace('ee._uu.', 'united_states')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a292d3",
   "metadata": {},
   "source": [
    "Ensure that date-related columns are in the appropriate datetime format for easier processing and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace underscores with spaces for the date columns\n",
    "df_cleaned['order_date_dateorders'] = df_cleaned['order_date_dateorders'].str.replace('_', ' ')\n",
    "df_cleaned['shipping_date_dateorders'] = df_cleaned['shipping_date_dateorders'].str.replace('_', ' ')\n",
    "\n",
    "# convert to datetime\n",
    "df_cleaned['order_date_dateorders'] = pd.to_datetime(df_cleaned['order_date_dateorders'])\n",
    "df_cleaned['shipping_date_dateorders'] = pd.to_datetime(df_cleaned['shipping_date_dateorders'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2919027",
   "metadata": {},
   "source": [
    "Extract useful temporal features such as year, month, day, day of the week, and whether the date falls on a weekend. This helps in capturing potential patterns related to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for order_date_dateorders\n",
    "df_cleaned['order_year'] = df_cleaned['order_date_dateorders'].dt.year\n",
    "df_cleaned['order_month'] = df_cleaned['order_date_dateorders'].dt.month\n",
    "df_cleaned['order_day'] = df_cleaned['order_date_dateorders'].dt.day\n",
    "df_cleaned['order_dayofweek'] = df_cleaned['order_date_dateorders'].dt.dayofweek\n",
    "df_cleaned['order_is_weekend'] = df_cleaned['order_dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# for shipping_date_dateorders\n",
    "df_cleaned['shipping_year'] = df_cleaned['shipping_date_dateorders'].dt.year\n",
    "df_cleaned['shipping_month'] = df_cleaned['shipping_date_dateorders'].dt.month\n",
    "df_cleaned['shipping_day'] = df_cleaned['shipping_date_dateorders'].dt.day\n",
    "df_cleaned['shipping_dayofweek'] = df_cleaned['shipping_date_dateorders'].dt.dayofweek\n",
    "df_cleaned['shipping_is_weekend'] = df_cleaned['shipping_dayofweek'].isin([5, 6]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af57c0",
   "metadata": {},
   "source": [
    "Calculate the time difference between order and shipping dates, which provides insights into shipping duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['days_to_ship'] = (df_cleaned['shipping_date_dateorders'] - df_cleaned['order_date_dateorders']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the original dates columns\n",
    "df_cleaned.drop(['order_date_dateorders','shipping_date_dateorders'],axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f16ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ee65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset again\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the latitude and longitude columns\n",
    "df_cleaned.drop(['latitude','longitude'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2328109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dummies from the dataframe\n",
    "df_transformed = pd.get_dummies(df_cleaned, drop_first=True)\n",
    "\n",
    "# show the df again\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bed87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate skewness for numeric volumn to determine which one needs to be log_transformed\n",
    "skewness_values = df_transformed[['days_for_shipping_real', 'days_for_shipment_scheduled', 'benefit_per_order', \\\n",
    "                      'order_item_discount', 'order_item_discount_rate', 'order_item_profit_ratio', \\\n",
    "                      'order_item_quantity', 'sales']].skew()\n",
    "print(skewness_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d00e1",
   "metadata": {},
   "source": [
    "Based on the skewness values , the following columns are candidates for log transformation:\n",
    "\n",
    "- **benefit_per_order:** Strong negative skewness, reducing influence of larger values.\n",
    "- **order_item_discount:** Positive skewness, normalizing large discount outliers.\n",
    "- **order_item_profit_ratio:** Negative skewness, normalizing smaller values.\n",
    "- **sales:** Positive skewness, normalizing larger values.\n",
    "\n",
    "\n",
    "Transforming these columns can better align our data with modeling assumptions and possibly improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_log = ['benefit_per_order', 'order_item_discount', 'order_item_profit_ratio',\\\n",
    "                  'sales','order_item_quantity','days_for_shipment_scheduled']\n",
    "\n",
    "for col in columns_to_log:\n",
    "    df_transformed[col] = df_transformed[col].apply(lambda x: np.log1p(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values again after log transformation\n",
    "df_transformed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null values\n",
    "df_transformed.dropna(axis = 0, how = 'any', inplace = True)\n",
    "#drop sales_per_customer to avoid multicollinearity\n",
    "df_transformed.drop('sales_per_customer',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the transformed data info\n",
    "df_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb44dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.corr().sales.abs().sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6519a",
   "metadata": {},
   "source": [
    "## E. Sales Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function module\n",
    "from Function.Project_VNg63984_Function import select_manual, select_variance, select_best, make_poly, \\\n",
    "perform_linear_regression, perform_ridge_regression, perform_lasso_regression, calculate_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e0b45",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835428c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set target\n",
    "target = 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace +inf and -inf with NaN\n",
    "df_transformed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "df_transformed.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data first\n",
    "scaler = StandardScaler()\n",
    "df_transformed_scaled = pd.DataFrame(scaler.fit_transform(df_transformed), columns=df_transformed.columns)\n",
    "\n",
    "# variance threshold selection\n",
    "threshold_variance = 0.1\n",
    "df_variance = select_variance(df_transformed_scaled, target, threshold_variance)\n",
    "df_variance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Best selection\n",
    "\n",
    "X = df_transformed_scaled.drop(target, axis=1)\n",
    "y = df_transformed_scaled[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "selected_features_kbest = select_best(X_train, y_train, 12)\n",
    "df_selKBest = df_transformed_scaled[selected_features_kbest]\n",
    "\n",
    "df_selKBest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db914dc",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty placeholder lists to store experiment's results\n",
    "\n",
    "fselection_list = []\n",
    "ftransform_list = []\n",
    "r2_list = []\n",
    "rmse_list = []\n",
    "model_list = []\n",
    "X_test_list = []\n",
    "Y_test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57546003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through different feature selection methods\n",
    "# using  80:20 for training and testing and random_state=42\n",
    "\n",
    "for df_selection in [df_variance, df_selKBest]:\n",
    "    # function for linear regression with or without polynomial features.\n",
    "    # returns R-squared, RMSE, model, X_test, and Y_test.  \n",
    "    \n",
    "    # linear Regression without Polynomial Features\n",
    "\n",
    "    r2, rmse, model, X_test, Y_test = perform_linear_regression(df_selection,df_transformed[target],test_size = 0.2 ,random_state = 42)\n",
    "    \n",
    "    fselection_list.append('Variance' if df_selection is df_variance else 'SelectKBest')\n",
    "    ftransform_list.append('None')\n",
    "    r2_list.append(r2)\n",
    "    rmse_list.append(rmse)\n",
    "    model_list.append(model)\n",
    "    X_test_list.append(X_test)\n",
    "    Y_test_list.append(Y_test)\n",
    "\n",
    "    # linear Regression with Polynomial Features\n",
    "    r2, rmse, model, X_test, Y_test = perform_linear_regression(df_selection, df_transformed[target],test_size = 0.2 ,\n",
    "                                                                random_state = 42, transformation='Poly 2 Interaction')\n",
    "    \n",
    "    fselection_list.append('Variance' if df_selection is df_variance else 'SelectKBest')\n",
    "    ftransform_list.append('Poly 2 Interaction')\n",
    "    r2_list.append(r2)\n",
    "    rmse_list.append(rmse)\n",
    "    model_list.append(model)\n",
    "    X_test_list.append(X_test)\n",
    "    Y_test_list.append(Y_test)\n",
    "    \n",
    "    \n",
    "    # linear Regression with pca\n",
    "    r2, rmse, model, X_test, Y_test = perform_linear_regression(df_selection, df_transformed[target] ,test_size = 0.2 ,\n",
    "                                                                random_state = 42, transformation='PCA')\n",
    "    \n",
    "    fselection_list.append('Variance' if df_selection is df_variance else 'SelectKBest')\n",
    "    ftransform_list.append('PCA')\n",
    "    r2_list.append(r2)\n",
    "    rmse_list.append(rmse)\n",
    "    model_list.append(model)\n",
    "    X_test_list.append(X_test)\n",
    "    Y_test_list.append(Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c758c",
   "metadata": {},
   "source": [
    "#### Linear Regression Model with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 80:20 for training and testing and random_state=42\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_transformed.drop(target, axis=1), df_transformed[target], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an array with alpha values\n",
    "alphas = 10**np.linspace(5, -2, 15)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty placeholder lists to store R2 and RMSE values for each alpha\n",
    "ridge_r2_list = []\n",
    "ridge_rmse_list = []\n",
    "ridge_model_list = []\n",
    "ridge_X_test = []\n",
    "ridge_Y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the ridge regression for each alpha value\n",
    "for alpha in alphas:\n",
    "    r2, rmse, ridge, X_test, Y_test = perform_ridge_regression(X_train, X_test, Y_train, Y_test, alpha)\n",
    "    ridge_r2_list.append(r2)\n",
    "    ridge_rmse_list.append(rmse)\n",
    "    ridge_model_list.append(ridge)\n",
    "    ridge_X_test.append(X_test)\n",
    "    ridge_Y_test.append(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda9d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display the ridge result\n",
    "ridge_result = np.vstack((alphas, ridge_rmse_list, ridge_r2_list)).T\n",
    "ridge_df = pd.DataFrame(ridge_result, columns=['Alpha', 'RMSE', \"R2\"])\n",
    "ridge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47008eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the alpha value with the minimum RMSE score\n",
    "ridge_df_sorted = ridge_df.sort_values(by=['RMSE', 'R2'], ascending=[True, False])\n",
    "ridge_df_sorted.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fab3b",
   "metadata": {},
   "source": [
    "The best alpha value to choose for Ridge regression is 10. It resulted in the lowest RMSE of 0.185 and a higher R-squared value of 0.925, indicating a better model fit compared to other alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c78f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the best alpha\n",
    "best_alpha_ridge = ridge_df_sorted.iloc[0]['Alpha']\n",
    "best_alpha_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e093d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the results to the placeholder lists\n",
    "fselection_list.append(f\"Ridge, alpha = {best_alpha_ridge}\")\n",
    "ftransform_list.append('None')\n",
    "r2_list.append(ridge_df_sorted.iloc[0]['R2'])\n",
    "rmse_list.append(ridge_df_sorted.iloc[0]['RMSE'])\n",
    "model_list.append(ridge_model_list[8])\n",
    "X_test_list.append(ridge_X_test[8])\n",
    "Y_test_list.append(ridge_Y_test[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e92192",
   "metadata": {},
   "source": [
    "#### Linear Regression Model with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ca0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_transformed.drop(target, axis=1), df_transformed[target], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty placeholder lists to store R2 and RMSE values for each alpha\n",
    "lasso_r2_list = []\n",
    "lasso_rmse_list = []\n",
    "lasso_model_list = []\n",
    "lasso_X_test = []\n",
    "lasso_Y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea114a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the lasso regression for each alpha value\n",
    "\n",
    "for alpha in alphas:\n",
    "    r2, rmse, lasso, X_test, Y_test = perform_lasso_regression(X_train, X_test, Y_train, Y_test, alpha)\n",
    "    lasso_r2_list.append(r2)\n",
    "    lasso_rmse_list.append(rmse)\n",
    "    lasso_model_list.append(lasso)\n",
    "    lasso_X_test.append(X_test)\n",
    "    lasso_Y_test.append(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5084ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the lasso resulta\n",
    "lasso_df = pd.DataFrame(zip(alphas, lasso_rmse_list, lasso_r2_list), columns=['Alpha', 'RMSE', \"R2\"])\n",
    "lasso_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_df.sort_values(by=['RMSE', 'R2'], ascending=[True, False]).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf528182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the best alpha\n",
    "best_alpha_lasso = lasso_df.iloc[14]['Alpha']\n",
    "best_alpha_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c7416",
   "metadata": {},
   "source": [
    "The best alpha value to choose for Lasso regression is 0.01. It resulted in the lowest RMSE of 0.486 and a higher R-squared value of 0.878, indicating a better model fit compared to other alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08407043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the results to the placeholder lists\n",
    "fselection_list.append(f\"Lasso, alpha = {best_alpha_lasso}\")\n",
    "ftransform_list.append('None')\n",
    "r2_list.append(lasso_df.iloc[14]['R2'])\n",
    "rmse_list.append(lasso_df.iloc[14]['RMSE'])\n",
    "model_list.append(lasso_model_list[14])\n",
    "X_test_list.append(lasso_X_test[14])\n",
    "Y_test_list.append(lasso_Y_test[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round R2 and RMSE values to 2 decimal places\n",
    "r2_list = [round(float(r2), 3) if r2 > 0 else 'N/A' for r2 in r2_list]\n",
    "rmse_list = [round(float(rmse), 3) if rmse < 10000 else 'N/A' for rmse in rmse_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94221a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all lists to a numpy array\n",
    "data = np.vstack((fselection_list, ftransform_list, r2_list, rmse_list)).T\n",
    "\n",
    "# create a dataframe from the numpy array\n",
    "results_df = pd.DataFrame(data, columns=['Feature Selection', 'Feature Transformation','R2', 'RMSE'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b925b",
   "metadata": {},
   "source": [
    "Based on the results from the different models and feature transformations, the best-performing model is the Linear Regression with variance feature selation and poly transformation, achieving an R2 value of approximately 0.99 and an RMSE of approximately 0.06. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_best = model_list[1].predict(X_test_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scatter plot to compare the output of the model (Y_pred) and the test dataset (Y_test)\n",
    "\n",
    "plt.scatter(Y_test_list[1], Y_pred_best)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Scatter Plot of Predicted vs Actual Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f8d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851781d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d682c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb029dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e38d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49a6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa19ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920a62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed2aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed576c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7433cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb48ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c29aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c33de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31378775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf523c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d7ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896643bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc5378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a6bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74efe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d3582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f49ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
